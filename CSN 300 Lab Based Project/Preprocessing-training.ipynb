{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load all the data into suitable dataframe using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data = pd.read_csv(\"./survival_data.csv\")\n",
    "flair_data = pd.read_csv(\"./TrainingDataHGGFLAIR.csv\")\n",
    "t1ce_data = pd.read_csv(\"./TrainingDataHGGT1CE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data1 = pd.merge(clinical_data, flair_data, on='folder_name')\n",
    "merge_data2 = pd.merge(merge_data1, t1ce_data, on='folder_name')\n",
    "#merge_data2.to_csv(\"./new csvs/preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract integer and float datatype features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "float_var = merge_data2.dtypes[merge_data2.dtypes=='float'].index.values\n",
    "int_var = merge_data2.dtypes[merge_data2.dtypes=='int'].index.values\n",
    "\n",
    "f_list = [x for x in float_var]\n",
    "i_list = [x for x in int_var]\n",
    "use_features = f_list + i_list\n",
    "print(len(use_features))\n",
    "\n",
    "data_used = merge_data2[use_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_used.to_csv((\"./new csvs/use_features_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising data using MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_normalised = pd.DataFrame(min_max_scaler.fit_transform(data_used), columns=data_used[use_features].columns, index=data_used[use_features].index)\n",
    "#data_normalised.to_csv(\"./new csvs/normalised_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label = 'Survival'\n",
    "features = [x for x in use_features if x not in label]\n",
    "b = data_normalised[label]  #data_labels\n",
    "a = data_normalised[features]\n",
    "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = svm.SVR(kernel='rbf')\n",
    "clf.fit(a_train, b_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted = clf.predict(a_test)\n",
    "print(r2_score(b_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rFG = RandomForestRegressor(max_depth = 2, random_state=0, n_estimators = 100)\n",
    "rFG.fit(a_train, b_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted_rfg = clf.predict(a_test)\n",
    "print(r2_score(b_test,predicted_rfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(a_train, b_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted_lr1 = lr.predict(a_test)\n",
    "print(r2_score(b_test,predicted_lr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.feature_selection import SelectKBest, f_regression\\ndata_used.shape\\ntest = SelectKBest(score_func=f_regression, k=50)\\nfit = test.fit(a_train, b_train)\\nnp.set_printoptions(precision=3)\\n#print(fit.scores_)\\n\\nfeatures_ft = fit.transform(a_train)\\nprint(features_ft[0:5])\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "data_used.shape\n",
    "test = SelectKBest(score_func=f_regression, k=50)\n",
    "fit = test.fit(a_train, b_train)\n",
    "np.set_printoptions(precision=3)\n",
    "#print(fit.scores_)\n",
    "\n",
    "features_ft = fit.transform(a_train)\n",
    "print(features_ft[0:5])\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank the features for their importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "#selector = RFE(lr, 212, 1, 0)     #for linear regression\n",
    "selector = RFE(rFG, 212, 1, 0)\n",
    "fitting = selector.fit(a_train, b_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_f = fitting.support_\n",
    "all_f = a_train.keys()\n",
    "rank_f = fitting.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b = {}\n",
    "\n",
    "for i in range(0, len(selected_f)):\n",
    "    #print(selected_f[i])\n",
    "    if(selected_f[i]==True) :\n",
    "        b[all_f[i]]=rank_f[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z score normalistation\n",
    "\n",
    "0 mean and unit standard deviation\n",
    "\n",
    "svm non linear\n",
    "\n",
    "rank the features\n",
    "\n",
    "caclulate all the metrics as in the paper\n",
    "\n",
    "cross validation 5 fold and 10 fold\n",
    "\n",
    "report results for both of the above versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation using Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalising 'data_used' dataframe using standardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "data_standardised = pd.DataFrame(scaler.fit_transform(data_used), columns=data_used[use_features].columns, index=data_used[use_features].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_standardised.to_csv(\"Standardised_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into train and test features = (newa_train, newa_test)  labels = (newb_train, newb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Survival'\n",
    "features = [x for x in use_features if x not in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_b = data_standardised[label]  #data_labels\n",
    "new_a = data_standardised[features]\n",
    "#newa_train, newa_test, newb_train, newb_test = train_test_split(new_a, new_b, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing svm.SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR(kernel='poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_SE_5 = cross_val_score(clf, new_a, new_b, scoring = 'neg_median_absolute_error', cv=5)\n",
    "MSE_5 = cross_val_score(clf, new_a, new_b, scoring = 'neg_mean_squared_error', cv=5)\n",
    "explained_variance_5 = cross_val_score(clf, new_a, new_b, scoring = 'explained_variance', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising metrics of 5 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 Fold CV results \n",
      "\n",
      "(1, \" - fold's \", 'median_se= ', -0.6228583465154689, ' mse= ', -1.992738391335075, ' expl_var= ', -1.3250911048390122)\n",
      "(2, \" - fold's \", 'median_se= ', -0.7122153150671409, ' mse= ', -2.140214831669809, ' expl_var= ', -0.16414118812059897)\n",
      "(3, \" - fold's \", 'median_se= ', -0.5240161346636534, ' mse= ', -1.1408091259739441, ' expl_var= ', -0.44973264924839573)\n",
      "(4, \" - fold's \", 'median_se= ', -0.6081387032202278, ' mse= ', -1.1193101505209784, ' expl_var= ', 0.06764903897335472)\n",
      "(5, \" - fold's \", 'median_se= ', -0.40599081493109446, ' mse= ', -1.004456145549448, ' expl_var= ', -0.5518747022299224)\n"
     ]
    }
   ],
   "source": [
    "print('\\n 5 Fold CV results \\n')\n",
    "\n",
    "for i in range(0,5) :\n",
    "    print(i+1,' - fold\\'s ','median_se= ',median_SE_5[i],' mse= ', MSE_5[i],' expl_var= ',explained_variance_5[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnames = [0,1,2,3,4]\\ndata = np.array([names, median_SE_5, MSE_5, explained_variance_5])\\nprint(pd.DataFrame(data=data[1:,1:], index=data[1:,0], columns=data[0,1:]))\\nprint(median_SE_5)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "names = [0,1,2,3,4]\n",
    "data = np.array([names, median_SE_5, MSE_5, explained_variance_5])\n",
    "print(pd.DataFrame(data=data[1:,1:], index=data[1:,0], columns=data[0,1:]))\n",
    "print(median_SE_5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_SE_10 = cross_val_score(clf, new_a, new_b, scoring = 'neg_median_absolute_error', cv=10)\n",
    "MSE_10 = cross_val_score(clf, new_a, new_b, scoring = 'neg_mean_squared_error', cv=10)\n",
    "explained_variance_10 = cross_val_score(clf, new_a, new_b, scoring = 'explained_variance', cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising metrics of 10 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10-Fold CV results \n",
      "\n",
      "(1, \" - fold's \", 'median_se= ', -0.6245513356477137, ' mse= ', -2.7936863159641754, ' expl_var= ', -1.6245373025552388)\n",
      "(2, \" - fold's \", 'median_se= ', -0.444578811805348, ' mse= ', -0.8408877974864971, ' expl_var= ', 0.08249786324493902)\n",
      "(3, \" - fold's \", 'median_se= ', -0.6898435431192433, ' mse= ', -1.4982027528089334, ' expl_var= ', -0.08664065230384099)\n",
      "(4, \" - fold's \", 'median_se= ', -1.2342987836413526, ' mse= ', -2.5209530744085713, ' expl_var= ', -0.0551968690863629)\n",
      "(5, \" - fold's \", 'median_se= ', -0.3767825854855964, ' mse= ', -1.0822786985743569, ' expl_var= ', -2.5208708333294267)\n",
      "(6, \" - fold's \", 'median_se= ', -0.5542973896541944, ' mse= ', -1.2987493149820364, ' expl_var= ', -0.016354490503409203)\n",
      "(7, \" - fold's \", 'median_se= ', -0.4573685225108183, ' mse= ', -1.2087436490293366, ' expl_var= ', 0.19422338974331488)\n",
      "(8, \" - fold's \", 'median_se= ', -0.7385877075150559, ' mse= ', -0.9448893158693521, ' expl_var= ', -0.00439536835770693)\n",
      "(9, \" - fold's \", 'median_se= ', -0.35964603917663795, ' mse= ', -0.9238238712885446, ' expl_var= ', -2.6217367304197974)\n",
      "(10, \" - fold's \", 'median_se= ', -0.4208084757696591, ' mse= ', -1.1069271146309467, ' expl_var= ', -0.026330113397860178)\n"
     ]
    }
   ],
   "source": [
    "print('\\n 10-Fold CV results \\n')\n",
    "\n",
    "for i in range(0,10) :\n",
    "    print(i+1,' - fold\\'s ','median_se= ',median_SE_10[i],' mse= ', MSE_10[i],' expl_var= ',explained_variance_10[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
